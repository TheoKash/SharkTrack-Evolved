{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from viame_annotation import Viame\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viame_to_standard(csv_path, source):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_path, skiprows=lambda x: x in [1])\n",
    "    viame = Viame()\n",
    "\n",
    "    # Initialize a list to hold the data for each row\n",
    "    rows_list = []\n",
    "\n",
    "    # Iterate over the rows of the DataFrame and process each annotation\n",
    "    for index, row in df.iterrows():\n",
    "        # Build the Filename\n",
    "        filename = viame.get_frame_id(row, source)\n",
    "        track_id = viame.get_track_id(row)\n",
    "        # Extract Family, Genus, Species\n",
    "        family, genus, species = viame.get_taxonomy(row)\n",
    "        \n",
    "        # Extract bounding box coordinates\n",
    "        xmin, ymin, xmax, ymax = viame.get_bbox(row)\n",
    "        \n",
    "        # Prepare the new row as a Series\n",
    "        new_row = pd.Series({\n",
    "            'Filename': filename,\n",
    "            'Family': family,\n",
    "            'Genus': genus,\n",
    "            'Species': species,\n",
    "            'ymin': ymin,\n",
    "            'xmin': xmin,\n",
    "            'xmax': xmax,\n",
    "            'ymax': ymax,\n",
    "            'Augmentation': \"none\",\n",
    "            'Source': source,\n",
    "            'track_id': track_id,\n",
    "        })\n",
    "\n",
    "        # Append the new Series to the list\n",
    "        rows_list.append(new_row)\n",
    "\n",
    "    converted_df = pd.DataFrame(columns=['Filename', 'Family', 'Genus', 'Species', 'ymin', 'xmin', 'xmax', 'ymax', 'Augmentation', 'Source'])\n",
    "    # Concatenate all the Series into a new DataFrame\n",
    "    if len(rows_list) > 0:\n",
    "        converted_df = pd.concat(rows_list, axis=1).transpose()\n",
    "\n",
    "    # Write the converted DataFrame to a new CSV file\n",
    "    return converted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_to_viame(standard_df, original_csv_path, video_folder):\n",
    "  # TODO: THIS IS CHATGPT. CHECK BACK\n",
    "    # Read the original CSV to get the column names\n",
    "    original_df = pd.read_csv(original_csv_path, nrows=0)\n",
    "\n",
    "    # Create a new DataFrame with the same columns as the original\n",
    "    viame_df = pd.DataFrame(columns=original_df.columns)\n",
    "\n",
    "    # Iterate over the rows of the standard DataFrame and convert each back to the original format\n",
    "    for index, row in standard_df.iterrows():\n",
    "        # Rebuild the '10-11+: Repeated Species' field\n",
    "        repeated_species = f\"{row['Genus']} {row['Species']}\"\n",
    "        track_id = row['track_id']\n",
    "        \n",
    "        # Rebuild the '4-7: Img-bbox(TL_x,TL_y,BR_x,BR_y)' field\n",
    "        tl_x = row['xmin']\n",
    "        tl_y = row['ymin']\n",
    "        br_x = row['xmax']\n",
    "        br_y = row['ymax']\n",
    "        bbox = f\"{tl_x},{tl_y},{br_x},{br_y}\"\n",
    "        \n",
    "        # Rebuild the '3: Unique Frame Identifier' field\n",
    "        frame_id = int(row['Filename'].split('_frame')[1].split('.')[0]) // 30\n",
    "        \n",
    "        # Prepare the new row\n",
    "        new_row = {\n",
    "            '1: Detection or Track-id': \"\",  # Fill in or calculate as needed\n",
    "            '2: Video or Image Identifier': video_folder,  # Assuming video_folder is equivalent to this field\n",
    "            '3: Unique Frame Identifier': frame_id,\n",
    "            '4-7: Img-bbox(TL_x,TL_y,BR_x,BR_y)': bbox,\n",
    "            '8: Detection or Length Confidence': \"\",  # Fill in or calculate as needed\n",
    "            '9: Target Length (0 or -1 if invalid)': 0,  # Assuming default value\n",
    "            '10-11+: Repeated Species,Confidence Pairs or Attributes': repeated_species,\n",
    "            '# 1: Detection or Track-id': track_id,\n",
    "            # Add additional columns as needed\n",
    "        }\n",
    "\n",
    "        # Append the new row to the DataFrame\n",
    "        viame_df = viame_df.append(new_row, ignore_index=True)\n",
    "    \n",
    "    # Return the converted DataFrame\n",
    "    return viame_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting shlife_bull2.csv\n",
      "Converted shlife_bull2.csv\n",
      "Converting shlife_bull5.csv\n",
      "Converted shlife_bull5.csv\n",
      "Converting shlife_scalloped2.csv\n",
      "Converted shlife_scalloped2.csv\n",
      "Converting shlife_scalloped5.csv\n",
      "Converted shlife_scalloped5.csv\n",
      "Converting shlife_silvertip1.csv\n",
      "Converted shlife_silvertip1.csv\n",
      "Converting shlife_grey2.csv\n",
      "Converted shlife_grey2.csv\n",
      "Converting shlife_smooth1.csv\n",
      "Converted shlife_smooth1.csv\n",
      "Converting shlife_bull4.csv\n",
      "Converted shlife_bull4.csv\n",
      "Converting shlife_bull3.csv\n",
      "Converted shlife_bull3.csv\n",
      "Converting shlife_scalloped4.csv\n",
      "Converted shlife_scalloped4.csv\n",
      "Converting shlife_scalloped3.csv\n",
      "Converted shlife_scalloped3.csv\n",
      "Converting shlife_grey1.csv\n",
      "Converted shlife_grey1.csv\n",
      "Converting shlife_smooth2.csv\n",
      "Converted shlife_smooth2.csv\n",
      "Converting shlife_bull7.csv\n",
      "Converted shlife_bull7.csv\n",
      "Converting shlife_scalloped1.csv\n",
      "Converted shlife_scalloped1.csv\n",
      "Converting shlife_smooth3.csv\n",
      "Converted shlife_smooth3.csv\n",
      "Converting shlife_bull6.csv\n",
      "Converted shlife_bull6.csv\n",
      "Converting shlife_bull1.csv\n",
      "Converted shlife_bull1.csv\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the function\n",
    "base_dir = '/vol/biomedic3/bglocker/ugproj2324/fv220/datasets/frame_extraction_raw/shlife/cleaned_annotations/'\n",
    "annotation_folder = os.path.join(base_dir, 'annotations_viame')\n",
    "output_folder = os.path.join(base_dir, 'annotations_standard')\n",
    "\n",
    "for annotation in os.listdir(annotation_folder):\n",
    "  if annotation.endswith(\".csv\"):\n",
    "    annotation_name = annotation.split('.')[0]\n",
    "    print(f\"Converting {annotation}\")\n",
    "    # Convert the CSV file\n",
    "    standard_df = viame_to_standard(os.path.join(annotation_folder, annotation), annotation_name)\n",
    "    # Write the converted DataFrame to a new CSV file\n",
    "    standard_df.to_csv(os.path.join(output_folder, annotation), index=False)\n",
    "    print(f\"Converted {annotation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move Annotations to the folders in phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source val1_easy2 already exists, copying\n",
      "Source val1_medium2 already exists, copying\n",
      "Source val1_difficult2 already exists, copying\n",
      "Source val1_medium1 already exists, copying\n",
      "Source val1_difficult1 already exists, copying\n",
      "Source val1_easy1 already exists, copying\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "output_folder = '/vol/biomedic3/bglocker/ugproj2324/fv220/datasets/phase2'\n",
    "annotations_folders = ['/vol/biomedic3/bglocker/ugproj2324/fv220/datasets/validation/annotations_1fps/annotations_standard']\n",
    "\n",
    "for annotation_folder in annotations_folders:\n",
    "  for annotation in os.listdir(annotation_folder):\n",
    "    if annotation.endswith(\".csv\"):\n",
    "      source_name = annotation.split('.')[0]\n",
    "      if source_name in os.listdir(output_folder):\n",
    "        print(f\"Source {source_name} already exists, copying\")\n",
    "        # copy the file to the source folder\n",
    "        shutil.copy(os.path.join(annotation_folder, annotation), os.path.join(output_folder, source_name, annotation))\n",
    "      else:\n",
    "        print(f'not copying {source_name} as it does not exist in the output folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Negative Annotations from Phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For any annotation file in any subfolder of output_folder, make all the values in xmin, ymin, xmax, ymax columns be max(0, value)\n",
    "for root, dirs, files in os.walk(output_folder):\n",
    "  for file in files:\n",
    "    if file.endswith(\".csv\"):\n",
    "      print(f\"Processing {file}\")\n",
    "      df = pd.read_csv(os.path.join(root, file))\n",
    "      df['xmin'] = df['xmin'].apply(lambda x: max(0, x))\n",
    "      df['ymin'] = df['ymin'].apply(lambda x: max(0, x))\n",
    "      df['xmax'] = df['xmax'].apply(lambda x: max(0, x))\n",
    "      df['ymax'] = df['ymax'].apply(lambda x: max(0, x))\n",
    "      df.to_csv(os.path.join(root, file), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default_ml_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
