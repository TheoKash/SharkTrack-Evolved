{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from viame_annotation import Viame\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viame_to_standard(csv_path, source):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_path, skiprows=lambda x: x in [1]) # skip row if metadata\n",
    "    viame = Viame()\n",
    "\n",
    "    # Initialize a list to hold the data for each row\n",
    "    rows_list = []\n",
    "\n",
    "    # Iterate over the rows of the DataFrame and process each annotation\n",
    "    for index, row in df.iterrows():\n",
    "        # Build the Filename\n",
    "        frame_id = viame.get_frame_id(row)\n",
    "        filename = f\"{source}_frame{frame_id}.jpg\"\n",
    "        track_id = viame.get_track_id(row)\n",
    "        # Extract Family, Genus, Species\n",
    "        family, genus, species = viame.get_taxonomy(row)\n",
    "        \n",
    "        # Extract bounding box coordinates\n",
    "        xmin, ymin, xmax, ymax = viame.get_bbox(row)\n",
    "        \n",
    "        # Prepare the new row as a Series\n",
    "        new_row = pd.Series({\n",
    "            'Filename': filename,\n",
    "            'Family': family,\n",
    "            'Genus': genus,\n",
    "            'Species': species,\n",
    "            'ymin': ymin,\n",
    "            'xmin': xmin,\n",
    "            'xmax': xmax,\n",
    "            'ymax': ymax,\n",
    "            'Augmentation': \"none\",\n",
    "            'Source': source,\n",
    "            'track_id': track_id,\n",
    "            'frame_id': frame_id,\n",
    "        })\n",
    "\n",
    "        # Append the new Series to the list\n",
    "        rows_list.append(new_row)\n",
    "\n",
    "    converted_df = pd.DataFrame(columns=['Filename', 'Family', 'Genus', 'Species', 'ymin', 'xmin', 'xmax', 'ymax', 'Augmentation', 'Source', 'track_id', 'frame_id'])\n",
    "    # Concatenate all the Series into a new DataFrame\n",
    "    if len(rows_list) > 0:\n",
    "        converted_df = pd.concat(rows_list, axis=1).transpose()\n",
    "\n",
    "    # Write the converted DataFrame to a new CSV file\n",
    "    return converted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_to_viame(standard_df, original_csv_path, video_folder):\n",
    "  # TODO: THIS IS CHATGPT. CHECK BACK\n",
    "    # Read the original CSV to get the column names\n",
    "    original_df = pd.read_csv(original_csv_path, nrows=0)\n",
    "    \n",
    "    # Should be \n",
    "    print(len(standard_df))\n",
    "\n",
    "    # Create a new DataFrame with the same columns as the original\n",
    "    viame_df = pd.DataFrame(columns=original_df.columns)\n",
    "\n",
    "    # Iterate over the rows of the standard DataFrame and convert each back to the original format\n",
    "    for index, row in standard_df.iterrows():\n",
    "        # Rebuild the '10-11+: Repeated Species' field\n",
    "        repeated_species = f\"{row['Genus']} {row['Species']}\"\n",
    "        track_id = row['track_id']\n",
    "        \n",
    "        # Rebuild the '4-7: Img-bbox(TL_x,TL_y,BR_x,BR_y)' field\n",
    "        tl_x = row['xmin']\n",
    "        tl_y = row['ymin']\n",
    "        br_x = row['xmax']\n",
    "        br_y = row['ymax']\n",
    "        bbox = f\"{tl_x},{tl_y},{br_x},{br_y}\"\n",
    "        \n",
    "        # Rebuild the '3: Unique Frame Identifier' field\n",
    "        frame_id = int(row['Filename'].split('_frame')[1].split('.')[0]) // 30\n",
    "        \n",
    "        # Prepare the new row\n",
    "        new_row = {\n",
    "            '1: Detection or Track-id': \"\",  # Fill in or calculate as needed\n",
    "            '2: Video or Image Identifier': video_folder,  # Assuming video_folder is equivalent to this field\n",
    "            '3: Unique Frame Identifier': frame_id,\n",
    "            '4-7: Img-bbox(TL_x,TL_y,BR_x,BR_y)': bbox,\n",
    "            '8: Detection or Length Confidence': \"\",  # Fill in or calculate as needed\n",
    "            '9: Target Length (0 or -1 if invalid)': 0,  # Assuming default value\n",
    "            '10-11+: Repeated Species,Confidence Pairs or Attributes': repeated_species,\n",
    "            '# 1: Detection or Track-id': track_id,\n",
    "            # Add additional columns as needed\n",
    "        }\n",
    "\n",
    "        # Append the new row to the DataFrame\n",
    "        # viame_df = viame_df.append(new_row, ignore_index=True)\n",
    "        # DF has no attribute append \n",
    "        viame_df.loc[len(viame_df)] = new_row\n",
    "    \n",
    "    # Return the converted DataFrame\n",
    "    return viame_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "\n",
    "def convert_time(time_str):\n",
    "    \"\"\"\n",
    "    Convert YOLO time format (00h:02m:57s:510ms) to VIAME format (00:02:57.510000)\n",
    "    with proper millisecond to microsecond conversion (510ms â†’ 510000)\n",
    "    \"\"\"\n",
    "    # Extract components using regex\n",
    "    match = re.match(r'(\\d+)h:(\\d+)m:(\\d+)s:(\\d+)ms', time_str)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid time format: {time_str}\")\n",
    "    \n",
    "    hours, minutes, seconds, milliseconds = match.groups()\n",
    "    # Convert milliseconds to microseconds (6 digits) by multiplying by 1000\n",
    "    microseconds = int(milliseconds) * 1000\n",
    "    # Format as 6-digit zero-padded microseconds\n",
    "    return f\"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}.{microseconds:06d}\"\n",
    "\n",
    "def convert_yolo_to_viame(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Converts YOLO object detection CSV annotations to VIAME format.\n",
    "    Preserves track metadata as comments above each detection row.\n",
    "    Formats timestamp with proper microsecond conversion.\n",
    "    \"\"\"\n",
    "    with open(input_csv, 'r') as in_file, open(output_csv, 'w', newline='') as out_file:\n",
    "        # Write VIAME format header\n",
    "        out_file.write(\"# 1: Detection or Track-id,2: Video or Image Identifier,3: Unique Frame Identifier,\"\n",
    "                       \"4-7: Img-bbox(TL_x,TL_y,BR_x,BR_y),8: Detection or Length Confidence,\"\n",
    "                       \"9: Target Length (0 or -1 if invalid),10-11+: Repeated Species,Confidence Pairs or Attributes\\n\")\n",
    "        \n",
    "        reader = csv.DictReader(in_file)\n",
    "        for row in reader:\n",
    "            # Convert and format timestamp\n",
    "            viame_time = convert_time(row['time'])\n",
    "            \n",
    "            # Extract relevant fields\n",
    "            frame = int(float(row['frame']))\n",
    "            track_id = row['track_id']\n",
    "            species = row['label']\n",
    "            confidence = float(row['confidence'])\n",
    "            \n",
    "            # Process bounding box coordinates (round to integers)\n",
    "            xmin = round(float(row['xmin']))\n",
    "            ymin = round(float(row['ymin']))\n",
    "            xmax = round(float(row['xmax']))\n",
    "            ymax = round(float(row['ymax']))\n",
    "            \n",
    "            # Write track metadata as comment\n",
    "            out_file.write(f\"# track_metadata: {row['track_metadata']}\\n\")\n",
    "            \n",
    "            # Write VIAME-formatted detection line with corrected timestamp\n",
    "            out_file.write(f\"{track_id},{viame_time},{frame},{xmin},{ymin},{xmax},{ymax},\"\n",
    "                           f\"{confidence:.6f},-1,{species},{confidence:.6f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of convert yolo to viame\n",
    "\n",
    "yolo_csv = '/vol/biomedic3/bglocker/ugproj/tk1420/sharktrack/ALv4-v8n_augs_best/C09_240_LGX1_conf0.25/internal_results/output.csv'\n",
    "output_csv = '/vol/biomedic3/bglocker/ugproj/tk1420/SharkTrack-Dev/test_annotations_viame.csv'\n",
    "if os.path.exists(output_csv):\n",
    "    os.remove(os.path.join(output_csv))\n",
    "    \n",
    "convert_yolo_to_viame(yolo_csv, output_csv)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting annotations.viame.csv\n",
      "Converted annotations.csv\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the function\n",
    "# base_dir = '/vol/biomedic3/bglocker/ugproj2324/fv220/datasets/validation/annotations_10fps'\n",
    "# annotation_folder = os.path.join(base_dir, 'annotations_viame')\n",
    "# output_folder = os.path.join(base_dir, 'annotations_standard')\n",
    "base_dir = '/vol/biomedic3/bglocker/ugproj/tk1420/SharkTrack-Videos/difficult3/'\n",
    "annotation_folder = base_dir\n",
    "output_folder = base_dir\n",
    "\n",
    "for annotation in os.listdir(annotation_folder):\n",
    "  if annotation.endswith(\".csv\"):\n",
    "    annotation_name = annotation.split('.')[0]\n",
    "    print(f\"Converting {annotation}\")\n",
    "    # Convert the CSV file\n",
    "    standard_df = viame_to_standard(os.path.join(annotation_folder, annotation), annotation_name)\n",
    "    # Write the converted DataFrame to a new CSV file\n",
    "    annotation = 'annotations.csv'\n",
    "    standard_df.to_csv(os.path.join(output_folder, annotation), index=False)\n",
    "    print(f\"Converted {annotation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Family</th>\n",
       "      <th>Genus</th>\n",
       "      <th>Species</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>Augmentation</th>\n",
       "      <th>Source</th>\n",
       "      <th>track_id</th>\n",
       "      <th>frame_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sp_natgeo_frame30.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Carcharhinus</td>\n",
       "      <td>limbatus</td>\n",
       "      <td>251</td>\n",
       "      <td>1811</td>\n",
       "      <td>1921</td>\n",
       "      <td>340</td>\n",
       "      <td>none</td>\n",
       "      <td>sp_natgeo</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sp_natgeo_frame60.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Carcharhinus</td>\n",
       "      <td>limbatus</td>\n",
       "      <td>221</td>\n",
       "      <td>1650</td>\n",
       "      <td>1919</td>\n",
       "      <td>379</td>\n",
       "      <td>none</td>\n",
       "      <td>sp_natgeo</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sp_natgeo_frame90.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Carcharhinus</td>\n",
       "      <td>limbatus</td>\n",
       "      <td>253</td>\n",
       "      <td>1632</td>\n",
       "      <td>1749</td>\n",
       "      <td>410</td>\n",
       "      <td>none</td>\n",
       "      <td>sp_natgeo</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sp_natgeo_frame120.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Carcharhinus</td>\n",
       "      <td>limbatus</td>\n",
       "      <td>279</td>\n",
       "      <td>1614</td>\n",
       "      <td>1878</td>\n",
       "      <td>391</td>\n",
       "      <td>none</td>\n",
       "      <td>sp_natgeo</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sp_natgeo_frame150.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Carcharhinus</td>\n",
       "      <td>limbatus</td>\n",
       "      <td>276</td>\n",
       "      <td>1762</td>\n",
       "      <td>1919</td>\n",
       "      <td>364</td>\n",
       "      <td>none</td>\n",
       "      <td>sp_natgeo</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Filename  Family         Genus   Species  ymin  xmin  xmax  \\\n",
       "0   sp_natgeo_frame30.jpg     NaN  Carcharhinus  limbatus   251  1811  1921   \n",
       "1   sp_natgeo_frame60.jpg     NaN  Carcharhinus  limbatus   221  1650  1919   \n",
       "2   sp_natgeo_frame90.jpg     NaN  Carcharhinus  limbatus   253  1632  1749   \n",
       "3  sp_natgeo_frame120.jpg     NaN  Carcharhinus  limbatus   279  1614  1878   \n",
       "4  sp_natgeo_frame150.jpg     NaN  Carcharhinus  limbatus   276  1762  1919   \n",
       "\n",
       "   ymax Augmentation     Source  track_id  frame_id  \n",
       "0   340         none  sp_natgeo         1         1  \n",
       "1   379         none  sp_natgeo         1         2  \n",
       "2   410         none  sp_natgeo         1         3  \n",
       "3   391         none  sp_natgeo         1         4  \n",
       "4   364         none  sp_natgeo         1         5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(output_folder + '/sp_natgeo.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move Annotations to the folders in phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sp_palau already exists, copying\n",
      "Source sp_palau3 already exists, copying\n",
      "Source sp_palau4 already exists, copying\n",
      "Source sp_palau5 already exists, copying\n",
      "Source sp_palau2 already exists, copying\n",
      "Source shlife_smooth1 already exists, copying\n",
      "Source shlife_scalloped1 already exists, copying\n",
      "Source shlife_bull6 already exists, copying\n",
      "Source shlife_bull1 already exists, copying\n",
      "Source shlife_grey1 already exists, copying\n",
      "Source shlife_bull7 already exists, copying\n",
      "Source shlife_silvertip1 already exists, copying\n",
      "Source shlife_grey2 already exists, copying\n",
      "Source shlife_bull4 already exists, copying\n",
      "Source shlife_bull3 already exists, copying\n",
      "Source shlife_smooth3 already exists, copying\n",
      "Source shlife_scalloped4 already exists, copying\n",
      "Source shlife_scalloped3 already exists, copying\n",
      "Source shlife_scalloped2 already exists, copying\n",
      "Source shlife_smooth2 already exists, copying\n",
      "Source shlife_scalloped5 already exists, copying\n",
      "Source shlife_bull2 already exists, copying\n",
      "Source shlife_bull5 already exists, copying\n",
      "Source gfp_cuba1 already exists, copying\n",
      "Source gfp_nwa1 already exists, copying\n",
      "Source gfp_ferdinand1 already exists, copying\n",
      "Source gfp_rand5 already exists, copying\n",
      "Source gfp_rand2 already exists, copying\n",
      "Source gfp_mozambique1 already exists, copying\n",
      "Source gfp_jamaica1 already exists, copying\n",
      "Source gfp_caicos1 already exists, copying\n",
      "Source gfp_rand3 already exists, copying\n",
      "Source gfp_madagascar1 already exists, copying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source gfp_palau1 already exists, copying\n",
      "Source gfp_rand4 already exists, copying\n",
      "Source gfp_kiribati1 already exists, copying\n",
      "Source gfp_bahamas2 already exists, copying\n",
      "Source gfp_samoa1 already exists, copying\n",
      "Source gfp_maldives1 already exists, copying\n",
      "Source gfp_niue1 already exists, copying\n",
      "Source gfp_bahamas1 already exists, copying\n",
      "Source gfp_belize1 already exists, copying\n",
      "Source gfp_caledonia1 already exists, copying\n",
      "Source gfp_polynesia1 already exists, copying\n",
      "Source gfp_solomon1 already exists, copying\n",
      "Source gfp_rand9 already exists, copying\n",
      "Source gfp_rand11 already exists, copying\n",
      "Source gfp_rand7 already exists, copying\n",
      "Source gfp_fiji1 already exists, copying\n",
      "Source gfp_montserrat1 already exists, copying\n",
      "Source gfp_png1 already exists, copying\n",
      "Source gfp_barbados1 already exists, copying\n",
      "Source gfp_tobago1 already exists, copying\n",
      "Source gfp_tiger1 already exists, copying\n",
      "Source gfp_cook1 already exists, copying\n",
      "Source gfp_hawaii1 already exists, copying\n",
      "Source gfp_nwa2 already exists, copying\n",
      "Source gfp_rand1 already exists, copying\n",
      "Source gfp_tonga1 already exists, copying\n",
      "Source gfp_rand10 already exists, copying\n",
      "Source gfp_rand6 already exists, copying\n",
      "Source gfp_rand8 already exists, copying\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "output_folder = '/vol/biomedic3/bglocker/ugproj2324/fv220/datasets/phase2_copy'\n",
    "base_folder = '/vol/biomedic3/bglocker/ugproj2324/fv220/datasets/frame_extraction_raw/'\n",
    "annotations_folders = [\n",
    "  base_folder + 'sp/cleaned_annotations/annotations_standard',\n",
    "  base_folder + 'shlife/cleaned_annotations/annotations_standard',\n",
    "  base_folder + 'gfp/cleaned_annotations/annotations_standard'\n",
    "]\n",
    "\n",
    "for annotation_folder in annotations_folders:\n",
    "  for annotation in os.listdir(annotation_folder):\n",
    "    if annotation.endswith(\".csv\"):\n",
    "      source_name = annotation.split('.')[0]\n",
    "      if source_name in os.listdir(output_folder):\n",
    "        print(f\"Source {source_name} already exists, copying\")\n",
    "        # copy the file to the source folder\n",
    "        shutil.copy(os.path.join(annotation_folder, annotation), os.path.join(output_folder, source_name, annotation))\n",
    "      else:\n",
    "        print(f'not copying {source_name} as it does not exist in the output folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Negative Annotations from Phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# For any annotation file in any subfolder of output_folder, make all the values in xmin, ymin, xmax, ymax columns be max(0, value)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/vol/biomedic3/bglocker/ugproj2324/fv220/datasets/phase2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m root, dirs, files \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mwalk(output_folder):\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# For any annotation file in any subfolder of output_folder, make all the values in xmin, ymin, xmax, ymax columns be max(0, value)\n",
    "output_folder = '/vol/biomedic3/bglocker/ugproj2324/fv220/datasets/phase2'\n",
    "for root, dirs, files in os.walk(output_folder):\n",
    "  for file in files:\n",
    "    if file.endswith(\".csv\"):\n",
    "      print(f\"Processing {file}\")\n",
    "      df = pd.read_csv(os.path.join(root, file))\n",
    "      df['xmin'] = df['xmin'].apply(lambda x: max(0, x))\n",
    "      df['ymin'] = df['ymin'].apply(lambda x: max(0, x))\n",
    "      df['xmax'] = df['xmax'].apply(lambda x: max(0, x))\n",
    "      df['ymax'] = df['ymax'].apply(lambda x: max(0, x))\n",
    "      df.to_csv(os.path.join(root, file), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
