{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from viame_annotation import Viame\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viame_to_standard(csv_path, source):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_path, skiprows=lambda x: x in [1])\n",
    "    viame = Viame()\n",
    "\n",
    "    # Initialize a list to hold the data for each row\n",
    "    rows_list = []\n",
    "\n",
    "    # Iterate over the rows of the DataFrame and process each annotation\n",
    "    for index, row in df.iterrows():\n",
    "        # Build the Filename\n",
    "        filename = viame.get_id(row, source)\n",
    "        # Extract Family, Genus, Species\n",
    "        family, genus, species = viame.get_taxonomy(row)\n",
    "        \n",
    "        # Extract bounding box coordinates\n",
    "        xmin, ymin, xmax, ymax = viame.get_bbox(row)\n",
    "        \n",
    "        # Prepare the new row as a Series\n",
    "        new_row = pd.Series({\n",
    "            'Filename': filename,\n",
    "            'Family': family,\n",
    "            'Genus': genus,\n",
    "            'Species': species,\n",
    "            'ymin': ymin,\n",
    "            'xmin': xmin,\n",
    "            'xmax': xmax,\n",
    "            'ymax': ymax,\n",
    "            'Augmentation': \"none\",\n",
    "            'Source': source\n",
    "        })\n",
    "\n",
    "        # Append the new Series to the list\n",
    "        rows_list.append(new_row)\n",
    "\n",
    "    # Concatenate all the Series into a new DataFrame\n",
    "    converted_df = pd.concat(rows_list, axis=1).transpose()\n",
    "\n",
    "    # Write the converted DataFrame to a new CSV file\n",
    "    return converted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_to_viame(standard_df, original_csv_path, video_folder):\n",
    "  # TODO: THIS IS CHATGPT. CHECK BACK\n",
    "    # Read the original CSV to get the column names\n",
    "    original_df = pd.read_csv(original_csv_path, nrows=0)\n",
    "\n",
    "    # Create a new DataFrame with the same columns as the original\n",
    "    viame_df = pd.DataFrame(columns=original_df.columns)\n",
    "\n",
    "    # Iterate over the rows of the standard DataFrame and convert each back to the original format\n",
    "    for index, row in standard_df.iterrows():\n",
    "        # Rebuild the '10-11+: Repeated Species' field\n",
    "        repeated_species = f\"{row['Genus']} {row['Species']}\"\n",
    "        \n",
    "        # Rebuild the '4-7: Img-bbox(TL_x,TL_y,BR_x,BR_y)' field\n",
    "        tl_x = row['xmin']\n",
    "        tl_y = row['ymin']\n",
    "        br_x = row['xmax']\n",
    "        br_y = row['ymax']\n",
    "        bbox = f\"{tl_x},{tl_y},{br_x},{br_y}\"\n",
    "        \n",
    "        # Rebuild the '3: Unique Frame Identifier' field\n",
    "        frame_id = int(row['Filename'].split('_frame')[1].split('.')[0]) // 30\n",
    "        \n",
    "        # Prepare the new row\n",
    "        new_row = {\n",
    "            '1: Detection or Track-id': \"\",  # Fill in or calculate as needed\n",
    "            '2: Video or Image Identifier': video_folder,  # Assuming video_folder is equivalent to this field\n",
    "            '3: Unique Frame Identifier': frame_id,\n",
    "            '4-7: Img-bbox(TL_x,TL_y,BR_x,BR_y)': bbox,\n",
    "            '8: Detection or Length Confidence': \"\",  # Fill in or calculate as needed\n",
    "            '9: Target Length (0 or -1 if invalid)': 0,  # Assuming default value\n",
    "            '10-11+: Repeated Species,Confidence Pairs or Attributes': repeated_species\n",
    "            # Add additional columns as needed\n",
    "        }\n",
    "\n",
    "        # Append the new row to the DataFrame\n",
    "        viame_df = viame_df.append(new_row, ignore_index=True)\n",
    "    \n",
    "    # Return the converted DataFrame\n",
    "    return viame_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting shlife_grey1.csv\n",
      "Converted shlife_grey1.csv\n",
      "Converting shlife_bull7.csv\n",
      "Converted shlife_bull7.csv\n",
      "Converting shlife_scalloped5.csv\n",
      "Converted shlife_scalloped5.csv\n",
      "Converting shlife_scalloped2.csv\n",
      "Converted shlife_scalloped2.csv\n",
      "Converting shlife_smooth3.csv\n",
      "Converted shlife_smooth3.csv\n",
      "Converting shlife_bull1.csv\n",
      "Converted shlife_bull1.csv\n",
      "Converting shlife_bull6.csv\n",
      "Converted shlife_bull6.csv\n",
      "Converting shlife_silvertip1.csv\n",
      "Converted shlife_silvertip1.csv\n",
      "Converting shlife_smooth2.csv\n",
      "Converted shlife_smooth2.csv\n",
      "Converting shlife_scalloped3.csv\n",
      "Converted shlife_scalloped3.csv\n",
      "Converting shlife_scalloped4.csv\n",
      "Converted shlife_scalloped4.csv\n",
      "Converting shlife_smooth1.csv\n",
      "Converted shlife_smooth1.csv\n",
      "Converting shlife_bull5.csv\n",
      "Converted shlife_bull5.csv\n",
      "Converting shlife_bull2.csv\n",
      "Converted shlife_bull2.csv\n",
      "Converting shlife_scalloped1.csv\n",
      "Converted shlife_scalloped1.csv\n",
      "Converting shlife_grey2.csv\n",
      "Converted shlife_grey2.csv\n",
      "Converting shlife_bull3.csv\n",
      "Converted shlife_bull3.csv\n",
      "Converting shlife_bull4.csv\n",
      "Converted shlife_bull4.csv\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the function\n",
    "annotation_folder = '/vol/biomedic3/bglocker/ugproj2324/fv220/datasets/frame_extraction_raw/shlife_bruvs/annotations'\n",
    "output_folder = '/vol/biomedic3/bglocker/ugproj2324/fv220/datasets/frame_extraction_raw/shlife_bruvs/annotations_sharktrack'\n",
    "\n",
    "for annotation in os.listdir(annotation_folder):\n",
    "  if annotation.endswith(\".csv\"):\n",
    "    annotation_name = annotation.split('.')[0]\n",
    "    print(f\"Converting {annotation}\")\n",
    "    # Convert the CSV file\n",
    "    standard_df = viame_to_standard(os.path.join(annotation_folder, annotation), annotation_name)\n",
    "    # Write the converted DataFrame to a new CSV file\n",
    "    standard_df.to_csv(os.path.join(output_folder, annotation), index=False)\n",
    "    print(f\"Converted {annotation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move Annotations to the folders in phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source shlife_bull2 already exists, copying\n",
      "Source shlife_bull5 already exists, copying\n",
      "Source shlife_scalloped1 already exists, copying\n",
      "Source shlife_smooth1 already exists, copying\n",
      "Source shlife_bull4 already exists, copying\n",
      "Source shlife_bull3 already exists, copying\n",
      "Source shlife_grey2 already exists, copying\n",
      "Source shlife_smooth3 already exists, copying\n",
      "Source shlife_scalloped3 already exists, copying\n",
      "Source shlife_scalloped4 already exists, copying\n",
      "Source shlife_bull7 already exists, copying\n",
      "Source shlife_grey1 already exists, copying\n",
      "Source shlife_silvertip1 already exists, copying\n",
      "Source shlife_scalloped5 already exists, copying\n",
      "Source shlife_scalloped2 already exists, copying\n",
      "Source shlife_smooth2 already exists, copying\n",
      "Source shlife_bull6 already exists, copying\n",
      "Source shlife_bull1 already exists, copying\n",
      "Source gfp_tiger1 already exists, copying\n",
      "Source gfp_nwa2 already exists, copying\n",
      "Source gfp_rand4 already exists, copying\n",
      "Source gfp_rand3 already exists, copying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source gfp_caicos1 already exists, copying\n",
      "Source gfp_niue1 already exists, copying\n",
      "Source gfp_polynesia1 already exists, copying\n",
      "Source gfp_caledonia1 already exists, copying\n",
      "Source gfp_rand10 already exists, copying\n",
      "Source gfp_tonga1 already exists, copying\n",
      "Source gfp_png1 already exists, copying\n",
      "Source gfp_rand2 already exists, copying\n",
      "Source gfp_bahamas1 already exists, copying\n",
      "Source gfp_rand5 already exists, copying\n",
      "Source gfp_barbados1 already exists, copying\n",
      "Source gfp_cuba1 already exists, copying\n",
      "Source gfp_rand11 already exists, copying\n",
      "Source gfp_solomon1 already exists, copying\n",
      "Source gfp_montserrat1 already exists, copying\n",
      "Source gfp_palau1 already exists, copying\n",
      "Source gfp_madagascar1 already exists, copying\n",
      "Source gfp_tobago1 already exists, copying\n",
      "Source gfp_maldives1 already exists, copying\n",
      "Source gfp_cook1 already exists, copying\n",
      "Source gfp_samoa1 already exists, copying\n",
      "Source gfp_belize1 already exists, copying\n",
      "Source gfp_rand8 already exists, copying\n",
      "Source gfp_rand6 already exists, copying\n",
      "Source gfp_rand1 already exists, copying\n",
      "Source gfp_bahamas2 already exists, copying\n",
      "Source gfp_ferdinand1 already exists, copying\n",
      "Source gfp_kiribati1 already exists, copying\n",
      "Source gfp_mozambique1 already exists, copying\n",
      "Source gfp_hawaii1 already exists, copying\n",
      "Source gfp_jamaica1 already exists, copying\n",
      "Source gfp_fiji1 already exists, copying\n",
      "Source gfp_nwa1 already exists, copying\n",
      "Source gfp_rand7 already exists, copying\n",
      "Source gfp_rand9 already exists, copying\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "output_folder = '/vol/biomedic3/bglocker/ugproj2324/fv220/datasets/phase2'\n",
    "annotations_folders = ['/vol/biomedic3/bglocker/ugproj2324/fv220/datasets/frame_extraction_raw/shlife_bruvs/annotations_sharktrack', '/vol/biomedic3/bglocker/ugproj2324/fv220/datasets/frame_extraction_raw/gfp_bruvs/annotations_sharktrack']\n",
    "for annotation_folder in annotations_folders:\n",
    "  for annotation in os.listdir(annotation_folder):\n",
    "    if annotation.endswith(\".csv\"):\n",
    "      source_name = annotation.split('.')[0]\n",
    "      if source_name in os.listdir(output_folder):\n",
    "        print(f\"Source {source_name} already exists, copying\")\n",
    "        # copy the file to the source folder\n",
    "        shutil.copy(os.path.join(annotation_folder, annotation), os.path.join(output_folder, source_name, annotation))\n",
    "      else:\n",
    "        print(f'not copying {source_name} as it does not exist in the output folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default_ml_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
