{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from viame_annotation import Viame\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viame_to_standard(csv_path, source):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_path) #, skiprows=lambda x: x in [1]) # skip row if metadata\n",
    "    viame = Viame()\n",
    "\n",
    "    # Initialize a list to hold the data for each row\n",
    "    rows_list = []\n",
    "\n",
    "    # Iterate over the rows of the DataFrame and process each annotation\n",
    "    for index, row in df.iterrows():\n",
    "        # Build the Filename\n",
    "        filename = viame.get_frame_id(row, source)\n",
    "        track_id = viame.get_track_id(row)\n",
    "        # Extract Family, Genus, Species\n",
    "        family, genus, species = viame.get_taxonomy(row)\n",
    "        \n",
    "        # Extract bounding box coordinates\n",
    "        xmin, ymin, xmax, ymax = viame.get_bbox(row)\n",
    "        \n",
    "        # Prepare the new row as a Series\n",
    "        new_row = pd.Series({\n",
    "            'Filename': filename,\n",
    "            'Family': family,\n",
    "            'Genus': genus,\n",
    "            'Species': species,\n",
    "            'ymin': ymin,\n",
    "            'xmin': xmin,\n",
    "            'xmax': xmax,\n",
    "            'ymax': ymax,\n",
    "            'Augmentation': \"none\",\n",
    "            'Source': source,\n",
    "            'track_id': track_id,\n",
    "        })\n",
    "\n",
    "        # Append the new Series to the list\n",
    "        rows_list.append(new_row)\n",
    "\n",
    "    converted_df = pd.DataFrame(columns=['Filename', 'Family', 'Genus', 'Species', 'ymin', 'xmin', 'xmax', 'ymax', 'Augmentation', 'Source'])\n",
    "    # Concatenate all the Series into a new DataFrame\n",
    "    if len(rows_list) > 0:\n",
    "        converted_df = pd.concat(rows_list, axis=1).transpose()\n",
    "\n",
    "    # Write the converted DataFrame to a new CSV file\n",
    "    return converted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_to_viame(standard_df, original_csv_path, video_folder):\n",
    "  # TODO: THIS IS CHATGPT. CHECK BACK\n",
    "    # Read the original CSV to get the column names\n",
    "    original_df = pd.read_csv(original_csv_path, nrows=0)\n",
    "\n",
    "    # Create a new DataFrame with the same columns as the original\n",
    "    viame_df = pd.DataFrame(columns=original_df.columns)\n",
    "\n",
    "    # Iterate over the rows of the standard DataFrame and convert each back to the original format\n",
    "    for index, row in standard_df.iterrows():\n",
    "        # Rebuild the '10-11+: Repeated Species' field\n",
    "        repeated_species = f\"{row['Genus']} {row['Species']}\"\n",
    "        track_id = row['track_id']\n",
    "        \n",
    "        # Rebuild the '4-7: Img-bbox(TL_x,TL_y,BR_x,BR_y)' field\n",
    "        tl_x = row['xmin']\n",
    "        tl_y = row['ymin']\n",
    "        br_x = row['xmax']\n",
    "        br_y = row['ymax']\n",
    "        bbox = f\"{tl_x},{tl_y},{br_x},{br_y}\"\n",
    "        \n",
    "        # Rebuild the '3: Unique Frame Identifier' field\n",
    "        frame_id = int(row['Filename'].split('_frame')[1].split('.')[0]) // 30\n",
    "        \n",
    "        # Prepare the new row\n",
    "        new_row = {\n",
    "            '1: Detection or Track-id': \"\",  # Fill in or calculate as needed\n",
    "            '2: Video or Image Identifier': video_folder,  # Assuming video_folder is equivalent to this field\n",
    "            '3: Unique Frame Identifier': frame_id,\n",
    "            '4-7: Img-bbox(TL_x,TL_y,BR_x,BR_y)': bbox,\n",
    "            '8: Detection or Length Confidence': \"\",  # Fill in or calculate as needed\n",
    "            '9: Target Length (0 or -1 if invalid)': 0,  # Assuming default value\n",
    "            '10-11+: Repeated Species,Confidence Pairs or Attributes': repeated_species,\n",
    "            '# 1: Detection or Track-id': track_id,\n",
    "            # Add additional columns as needed\n",
    "        }\n",
    "\n",
    "        # Append the new row to the DataFrame\n",
    "        viame_df = viame_df.append(new_row, ignore_index=True)\n",
    "    \n",
    "    # Return the converted DataFrame\n",
    "    return viame_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting easy1.csv\n",
      "Converted easy1.csv\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the function\n",
    "base_dir = '/vol/biomedic3/bglocker/ugproj2324/fv220/datasets/validation/annotations_1fps/'\n",
    "annotation_folder = os.path.join(base_dir, 'annotations_viame')\n",
    "output_folder = os.path.join(base_dir, 'annotations_standard')\n",
    "\n",
    "for annotation in os.listdir(annotation_folder):\n",
    "  if annotation.endswith(\".csv\") and annotation == 'easy1.csv':\n",
    "    annotation_name = annotation.split('.')[0]\n",
    "    print(f\"Converting {annotation}\")\n",
    "    # Convert the CSV file\n",
    "    standard_df = viame_to_standard(os.path.join(annotation_folder, annotation), annotation_name)\n",
    "    # Write the converted DataFrame to a new CSV file\n",
    "    standard_df.to_csv(os.path.join(output_folder, annotation), index=False)\n",
    "    print(f\"Converted {annotation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Family</th>\n",
       "      <th>Genus</th>\n",
       "      <th>Species</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>Augmentation</th>\n",
       "      <th>Source</th>\n",
       "      <th>track_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shlife_bull6_frame300.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shark</td>\n",
       "      <td>NaN</td>\n",
       "      <td>185</td>\n",
       "      <td>800</td>\n",
       "      <td>1132</td>\n",
       "      <td>410</td>\n",
       "      <td>none</td>\n",
       "      <td>shlife_bull6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shlife_bull6_frame330.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shark</td>\n",
       "      <td>NaN</td>\n",
       "      <td>166</td>\n",
       "      <td>708</td>\n",
       "      <td>1079</td>\n",
       "      <td>403</td>\n",
       "      <td>none</td>\n",
       "      <td>shlife_bull6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shlife_bull6_frame360.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shark</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152</td>\n",
       "      <td>592</td>\n",
       "      <td>1034</td>\n",
       "      <td>407</td>\n",
       "      <td>none</td>\n",
       "      <td>shlife_bull6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shlife_bull6_frame390.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shark</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144</td>\n",
       "      <td>494</td>\n",
       "      <td>898</td>\n",
       "      <td>400</td>\n",
       "      <td>none</td>\n",
       "      <td>shlife_bull6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shlife_bull6_frame420.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shark</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114</td>\n",
       "      <td>430</td>\n",
       "      <td>913</td>\n",
       "      <td>424</td>\n",
       "      <td>none</td>\n",
       "      <td>shlife_bull6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Filename  Family  Genus  Species  ymin  xmin  xmax  ymax  \\\n",
       "0  shlife_bull6_frame300.jpg     NaN  shark      NaN   185   800  1132   410   \n",
       "1  shlife_bull6_frame330.jpg     NaN  shark      NaN   166   708  1079   403   \n",
       "2  shlife_bull6_frame360.jpg     NaN  shark      NaN   152   592  1034   407   \n",
       "3  shlife_bull6_frame390.jpg     NaN  shark      NaN   144   494   898   400   \n",
       "4  shlife_bull6_frame420.jpg     NaN  shark      NaN   114   430   913   424   \n",
       "\n",
       "  Augmentation        Source  track_id  \n",
       "0         none  shlife_bull6         1  \n",
       "1         none  shlife_bull6         1  \n",
       "2         none  shlife_bull6         1  \n",
       "3         none  shlife_bull6         1  \n",
       "4         none  shlife_bull6         1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(output_folder + '/shlife_bull6.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move Annotations to the folders in phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sp_palau already exists, copying\n",
      "Source sp_palau3 already exists, copying\n",
      "Source sp_palau4 already exists, copying\n",
      "Source sp_palau5 already exists, copying\n",
      "Source sp_palau2 already exists, copying\n",
      "Source shlife_smooth1 already exists, copying\n",
      "Source shlife_scalloped1 already exists, copying\n",
      "Source shlife_bull6 already exists, copying\n",
      "Source shlife_bull1 already exists, copying\n",
      "Source shlife_grey1 already exists, copying\n",
      "Source shlife_bull7 already exists, copying\n",
      "Source shlife_silvertip1 already exists, copying\n",
      "Source shlife_grey2 already exists, copying\n",
      "Source shlife_bull4 already exists, copying\n",
      "Source shlife_bull3 already exists, copying\n",
      "Source shlife_smooth3 already exists, copying\n",
      "Source shlife_scalloped4 already exists, copying\n",
      "Source shlife_scalloped3 already exists, copying\n",
      "Source shlife_scalloped2 already exists, copying\n",
      "Source shlife_smooth2 already exists, copying\n",
      "Source shlife_scalloped5 already exists, copying\n",
      "Source shlife_bull2 already exists, copying\n",
      "Source shlife_bull5 already exists, copying\n",
      "Source gfp_cuba1 already exists, copying\n",
      "Source gfp_nwa1 already exists, copying\n",
      "Source gfp_ferdinand1 already exists, copying\n",
      "Source gfp_rand5 already exists, copying\n",
      "Source gfp_rand2 already exists, copying\n",
      "Source gfp_mozambique1 already exists, copying\n",
      "Source gfp_jamaica1 already exists, copying\n",
      "Source gfp_caicos1 already exists, copying\n",
      "Source gfp_rand3 already exists, copying\n",
      "Source gfp_madagascar1 already exists, copying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source gfp_palau1 already exists, copying\n",
      "Source gfp_rand4 already exists, copying\n",
      "Source gfp_kiribati1 already exists, copying\n",
      "Source gfp_bahamas2 already exists, copying\n",
      "Source gfp_samoa1 already exists, copying\n",
      "Source gfp_maldives1 already exists, copying\n",
      "Source gfp_niue1 already exists, copying\n",
      "Source gfp_bahamas1 already exists, copying\n",
      "Source gfp_belize1 already exists, copying\n",
      "Source gfp_caledonia1 already exists, copying\n",
      "Source gfp_polynesia1 already exists, copying\n",
      "Source gfp_solomon1 already exists, copying\n",
      "Source gfp_rand9 already exists, copying\n",
      "Source gfp_rand11 already exists, copying\n",
      "Source gfp_rand7 already exists, copying\n",
      "Source gfp_fiji1 already exists, copying\n",
      "Source gfp_montserrat1 already exists, copying\n",
      "Source gfp_png1 already exists, copying\n",
      "Source gfp_barbados1 already exists, copying\n",
      "Source gfp_tobago1 already exists, copying\n",
      "Source gfp_tiger1 already exists, copying\n",
      "Source gfp_cook1 already exists, copying\n",
      "Source gfp_hawaii1 already exists, copying\n",
      "Source gfp_nwa2 already exists, copying\n",
      "Source gfp_rand1 already exists, copying\n",
      "Source gfp_tonga1 already exists, copying\n",
      "Source gfp_rand10 already exists, copying\n",
      "Source gfp_rand6 already exists, copying\n",
      "Source gfp_rand8 already exists, copying\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "output_folder = '/vol/biomedic3/bglocker/ugproj2324/fv220/datasets/phase2_copy'\n",
    "base_folder = '/vol/biomedic3/bglocker/ugproj2324/fv220/datasets/frame_extraction_raw/'\n",
    "annotations_folders = [\n",
    "  base_folder + 'sp/cleaned_annotations/annotations_standard',\n",
    "  base_folder + 'shlife/cleaned_annotations/annotations_standard',\n",
    "  base_folder + 'gfp/cleaned_annotations/annotations_standard'\n",
    "]\n",
    "\n",
    "for annotation_folder in annotations_folders:\n",
    "  for annotation in os.listdir(annotation_folder):\n",
    "    if annotation.endswith(\".csv\"):\n",
    "      source_name = annotation.split('.')[0]\n",
    "      if source_name in os.listdir(output_folder):\n",
    "        print(f\"Source {source_name} already exists, copying\")\n",
    "        # copy the file to the source folder\n",
    "        shutil.copy(os.path.join(annotation_folder, annotation), os.path.join(output_folder, source_name, annotation))\n",
    "      else:\n",
    "        print(f'not copying {source_name} as it does not exist in the output folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Negative Annotations from Phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n",
      "Processing annotations.csv\n"
     ]
    }
   ],
   "source": [
    "# For any annotation file in any subfolder of output_folder, make all the values in xmin, ymin, xmax, ymax columns be max(0, value)\n",
    "output_folder = '/vol/biomedic3/bglocker/ugproj2324/fv220/datasets/phase2_copy'\n",
    "for root, dirs, files in os.walk(output_folder):\n",
    "  for file in files:\n",
    "    if file.endswith(\".csv\"):\n",
    "      print(f\"Processing {file}\")\n",
    "      df = pd.read_csv(os.path.join(root, file))\n",
    "      df['xmin'] = df['xmin'].apply(lambda x: max(0, x))\n",
    "      df['ymin'] = df['ymin'].apply(lambda x: max(0, x))\n",
    "      df['xmax'] = df['xmax'].apply(lambda x: max(0, x))\n",
    "      df['ymax'] = df['ymax'].apply(lambda x: max(0, x))\n",
    "      df.to_csv(os.path.join(root, file), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default_ml_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
