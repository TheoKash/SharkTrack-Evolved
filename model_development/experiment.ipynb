{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Experimentation\n",
    "This notebook is designed to streamline experimentation with different models.\n",
    "\n",
    "1. Selecting a model, dataset, tracker and hyperparameters\n",
    "2. Training the model and evaluating both object detection and tracking performance\n",
    "3. Saving the results in wandb\n",
    "4. Storing the model and evaluation annotations in a folder\n",
    "5. **Investigation** of the results, detection of failure cases and model improvement\n",
    "6. **Iterating** over steps 1-5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from architectures import yolov8\n",
    "from trackers import botsort\n",
    "import wandb\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> run `wandb login` in terminal before running this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a model, dataset, tracker and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "architectures = {\n",
    "  \"yolov8\": yolov8.YoloV8\n",
    "  }\n",
    "trackers = {\n",
    "  'botsort': botsort.BotSort\n",
    "}\n",
    "dataset_mapping = {} # map dataset name to dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_train_params = [\"architecture\", \"data\", \"epochs\", \"batch_size\", \"img_size\", \"lr\", \"greyscale\", \"model_size\", \"model_path\"]\n",
    "required_eval_params = [\"conf_treshold\", \"eval_data\", \"iou_association_threshold\", \"tracker\"]\n",
    "dynamic = [\"pretrained\", \"annotation_path\"]\n",
    "\n",
    "def construct_hyperparameters(model_name, **kwargs):\n",
    "  \"\"\"\n",
    "  - train params: architecture, path, architecture, data, epochs, batch_size, img_size, train_mins, device\n",
    "  - eval params: conf_tresh, device, eval_data\n",
    "  \"\"\"\n",
    "  hyperparameters = {\n",
    "    \"model_name\": model_name\n",
    "  }\n",
    "\n",
    "  model_pretrained = False\n",
    "\n",
    "  with open(\"./assets/trained_models.json\", \"r\") as file:\n",
    "      trained_models = json.load(file)\n",
    "  assert trained_models is not None\n",
    "  \n",
    "  if model_name in trained_models:\n",
    "     model_pretrained = True\n",
    "     # Train params are already known  \n",
    "     model_train_params = trained_models[model_name]\n",
    "     assert model_train_params is not None and [param in model_train_params for param in required_train_params]\n",
    "     hyperparameters.update({param: model_train_params[param] for param in required_train_params})\n",
    "     hyperparameters.update({param: kwargs[param] for param in required_eval_params})\n",
    "\n",
    "  else:\n",
    "    # TODO: account for tf models\n",
    "    # TODO: allow for train_min, device s to be added afterwards\n",
    "    model_path = \"/vol/biomedic3/bglocker/ugproj2324/fv220/models/best.pt\"\n",
    "    assert len(kwargs) == len(required_train_params) + len(required_eval_params)\n",
    "    assert [param in kwargs for param in required_train_params + required_eval_params]\n",
    "    hyperparameters.update({**kwargs, \"path\": model_path})\n",
    "\n",
    "  hyperparameters['pretrained'] = model_pretrained\n",
    "  hyperparameters['annotations_path'] = \"/\".join(hyperparameters['model_path'].split(\"/\")[:-1]) + \"/annotations.csv\"\n",
    "\n",
    "  assert [param in hyperparameters for param in required_train_params + required_eval_params + dynamic]\n",
    "\n",
    "  return hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_experimentation(hyperparameters):\n",
    "  # run = wandb.init(project=\"SharkTrack\", config=hyperparameters)\n",
    "\n",
    "  try:\n",
    "    # dataset_name = hyperparameters['dataset_name']\n",
    "\n",
    "    tracker = trackers[hyperparameters['tracker']]()\n",
    "    model = architectures[hyperparameters['architecture']](hyperparameters, tracker)\n",
    "    # dataset = dataset_mapping[dataset_name]\n",
    "\n",
    "    # # model.fit(dataset)\n",
    "    mota, motp, idf1 =  model.evaluate()\n",
    "\n",
    "    # wandb.log({\"MOTA\": mota, \"MOTP\": motp, \"IDF1\": idf1})\n",
    "    # TODO: add image as well, test+time and test_device\n",
    "\n",
    "    # Update trained_models[hyperparameters[model_name]] with new hyperparameters\n",
    "    with open(\"./assets/trained_models.json\", \"r\") as file:\n",
    "        trained_models = json.load(file)\n",
    "        assert trained_models is not None\n",
    "        trained_models[hyperparameters['model_name']] = hyperparameters\n",
    "        trained_models['annotation_path'] = hyperparameters['annotations_path']\n",
    "        with open(\"./assets/trained_models.json\", \"w\") as file:\n",
    "          json.dump(trained_models, file)\n",
    "     \n",
    "  finally: \n",
    "    pass\n",
    "    # run.finish()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'yoloV8-medium-mvd2', 'architecture': 'yolov8', 'data': 'MVDv2', 'epochs': 50, 'batch_size': 16, 'img_size': 640, 'lr': 0.01, 'greyscale': True, 'model_size': 'm', 'model_path': '/vol/biomedic3/bglocker/ugproj2324/fv220/dev/shark_locator_tests/runs/detect/yolov8m_mvd2/best.pt', 'conf_treshold': 0.2, 'eval_data': 'eval1', 'iou_association_threshold': 0.5, 'tracker': 'botsort', 'pretrained': True, 'annotations_path': '/vol/biomedic3/bglocker/ugproj2324/fv220/dev/shark_locator_tests/runs/detect/yolov8m_mvd2/annotations.csv'}\n",
      "Initialised Model yoloV8-medium-mvd2 \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'YoloV8' object has no attribute 'greyscale_bruvs_videos_folder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m hyperparameters \u001b[38;5;241m=\u001b[39m construct_hyperparameters(model_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39meval_params)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(hyperparameters)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmodel_experimentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36mmodel_experimentation\u001b[0;34m(hyperparameters)\u001b[0m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m architectures[hyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchitecture\u001b[39m\u001b[38;5;124m'\u001b[39m]](hyperparameters, tracker)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# dataset = dataset_mapping[dataset_name]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# # model.fit(dataset)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m mota, motp, idf1 \u001b[38;5;241m=\u001b[39m  \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# wandb.log({\"MOTA\": mota, \"MOTP\": motp, \"IDF1\": idf1})\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# TODO: add image as well, test+time and test_device\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Update trained_models[hyperparameters[model_name]] with new hyperparameters\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./assets/trained_models.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/fv220/dev/scripts/model_development/architectures/yolov8.py:38\u001b[0m, in \u001b[0;36mYoloV8.evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     33\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m  1. Evaluate object detection model using the evaluation dataset\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m  2. Evaluate object detection model using the out-of-distribution evaluation dataset\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m  3. Evaluate tracker model using the evaluation dataset\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m   bruvs_video_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbruvs_videos_folder \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreyscale\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreyscale_bruvs_videos_folder\u001b[49m\n\u001b[1;32m     39\u001b[0m   videos \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(bruvs_video_folder)\n\u001b[1;32m     40\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m([video\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m video \u001b[38;5;129;01min\u001b[39;00m videos])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'YoloV8' object has no attribute 'greyscale_bruvs_videos_folder'"
     ]
    }
   ],
   "source": [
    "model_name = \"yoloV8-medium-mvd2\"\n",
    "eval_params = {\n",
    "  \"conf_treshold\": 0.2,\n",
    "  \"eval_data\": \"eval1\",\n",
    "  \"iou_association_threshold\": 0.5,\n",
    "  \"tracker\": \"botsort\"\n",
    "}\n",
    "\n",
    "hyperparameters = construct_hyperparameters(model_name, **eval_params)\n",
    "print(hyperparameters)\n",
    "\n",
    "model_experimentation(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default_ml_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
